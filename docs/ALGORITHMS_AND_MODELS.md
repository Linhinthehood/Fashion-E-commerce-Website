# THU·∫¨T TO√ÅN V√Ä M√î H√åNH - H·ªÜ TH·ªêNG RECOMMENDATION

## üìã M·ª§C L·ª§C
1. [T·ªïng quan](#1-t·ªïng-quan)
2. [M√¥ h√¨nh Deep Learning](#2-m√¥-h√¨nh-deep-learning)
3. [Thu·∫≠t to√°n Retrieval](#3-thu·∫≠t-to√°n-retrieval)
4. [Thu·∫≠t to√°n Scoring & Ranking](#4-thu·∫≠t-to√°n-scoring--ranking)
5. [Business Rules & Filtering](#5-business-rules--filtering)
6. [M√¥ h√¨nh v√† Thu·∫≠t to√°n s·∫Ω Tri·ªÉn khai](#6-m√¥-h√¨nh-v√†-thu·∫≠t-to√°n-s·∫Ω-tri·ªÉn-khai)
7. [So s√°nh v√† ƒê√°nh gi√°](#7-so-s√°nh-v√†-ƒë√°nh-gi√°)
8. [T√†i li·ªáu tham kh·∫£o](#8-t√†i-li·ªáu-tham-kh·∫£o)

---

## 1. T·ªîNG QUAN

### 1.1. Ki·∫øn tr√∫c t·ªïng th·ªÉ
H·ªá th·ªëng recommendation s·ª≠ d·ª•ng ki·∫øn tr√∫c **Two-Stage Retrieval & Ranking**:

```
Stage 1: Retrieval (Candidate Generation)
  ‚Üì
  [FAISS Vector Search] + [Popularity] + [User Affinity]
  ‚Üì
  ~50-200 candidates

Stage 2: Ranking (Candidate Scoring)
  ‚Üì
  [Business Rules] + [Soft Scoring] + [Diversification]
  ‚Üì
  Top-N recommendations
```

### 1.2. Ph√¢n lo·∫°i thu·∫≠t to√°n

| Lo·∫°i | Thu·∫≠t to√°n | Tr·∫°ng th√°i | M√¥ t·∫£ |
|------|-----------|------------|-------|
| **Deep Learning** | FashionCLIP | ‚úÖ ƒê√£ tri·ªÉn khai | Vision-language model ƒë·ªÉ encode h√¨nh ·∫£nh |
| **Vector Search** | FAISS (IndexFlatIP) | ‚úÖ ƒê√£ tri·ªÉn khai | Approximate Nearest Neighbors |
| **Similarity** | Cosine Similarity | ‚úÖ ƒê√£ tri·ªÉn khai | T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa embeddings |
| **Retrieval** | Hybrid Scoring | ‚úÖ ƒê√£ tri·ªÉn khai | K·∫øt h·ª£p embedding + popularity + affinity |
| **Ranking** | Business Rules | ‚úÖ ƒê√£ tri·ªÉn khai | Soft scoring v·ªõi price/gender/usage/brand |
| **Ranking** | Learning-to-Rank | ‚ùå Ch∆∞a tri·ªÉn khai | XGBoost/LightGBM cho ranking |
| **Diversification** | MMR/xQuAD | ‚ùå Ch∆∞a tri·ªÉn khai | Tr√°nh redundancy trong recommendations |
| **Testing** | A/B Testing | ‚ùå Ch∆∞a tri·ªÉn khai | So s√°nh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh |

---

## 2. M√î H√åNH DEEP LEARNING

### 2.1. FashionCLIP Model

#### 2.1.1. Ki·∫øn tr√∫c

**Base Model**: CLIP (Contrastive Language-Image Pre-training)
- **Provider**: OpenAI
- **Model**: `openai/clip-vit-base-patch32`
- **Vision Encoder**: Vision Transformer (ViT-Base/32)
  - Input: Images (224x224, RGB)
  - Output: Image embeddings (512 dimensions)
- **Text Encoder**: Transformer-based
  - Input: Text tokens (max 77 tokens)
  - Output: Text embeddings (512 dimensions)

**Customization**:
- **Projection Heads**: Linear layers ƒë·ªÉ gi·∫£m chi·ªÅu t·ª´ 512D ‚Üí 256D
- **Layer Normalization**: ·ªîn ƒë·ªãnh training v√† inference
- **L2 Normalization**: Normalize output embeddings (||embedding|| = 1)

#### 2.1.2. Forward Pass

```
Input Image (224x224, RGB)
  ‚Üì
Vision Transformer (ViT-Base/32)
  ‚Üì
Image Embedding (512D)
  ‚Üì
Projection Head (Linear + LayerNorm)
  ‚Üì
Fashion Embedding (256D)
  ‚Üì
L2 Normalization
  ‚Üì
Normalized Embedding (256D, ||embedding|| = 1.0)
```

#### 2.1.3. C√¥ng th·ª©c

**Image Embedding**:
```
e_img = L2_norm(ProjectionHead(ViT_Encoder(image)))
```

**Text Embedding**:
```
e_txt = L2_norm(ProjectionHead(Text_Encoder(text)))
```

Trong ƒë√≥:
- `ViT_Encoder`: Vision Transformer encoder
- `Text_Encoder`: Transformer text encoder
- `ProjectionHead`: Linear layer (512D ‚Üí 256D) + LayerNorm
- `L2_norm`: L2 normalization ƒë·ªÉ ||e|| = 1

#### 2.1.4. Training

**Checkpoint**: `models/fashion_clip_best.pt`

**Qu√° tr√¨nh training** (ƒë√£ th·ª±c hi·ªán):
1. **Base model**: Load pretrained CLIP t·ª´ OpenAI
2. **Fine-tuning**: Train tr√™n fashion dataset
3. **Projection heads**: Train projection layers ƒë·ªÉ map 512D ‚Üí 256D
4. **Objective**: Contrastive learning ƒë·ªÉ h·ªçc representation t·ªët h∆°n cho fashion domain

**Hyperparameters** (t·ª´ checkpoint config):
- `embedding_dim`: 256
- `max_length`: 77 (text tokens)
- `image_size`: 224
- `model_name`: "openai/clip-vit-base-patch32"

#### 2.1.5. Input/Output

| Component | Input | Output |
|-----------|-------|--------|
| **FashionCLIP.encode_image()** | PIL Image (224x224, RGB) | Vector 256D (float32, L2-normalized) |
| **FashionCLIP.encode_text()** | Text string (tokenized, max 77) | Vector 256D (float32, L2-normalized) |

**V√≠ d·ª•**:
```python
# Input
image = Image.open("product.jpg")  # PIL Image, 224x224, RGB

# Output
embedding = model.encode_image(image)
# Shape: (256,)
# Type: float32
# Norm: ||embedding|| = 1.0
```

---

## 3. THU·∫¨T TO√ÅN RETRIEVAL

### 3.1. FAISS Vector Search

#### 3.1.1. M√¥ t·∫£
**FAISS** (Facebook AI Similarity Search) l√† th∆∞ vi·ªán t·ªëi ∆∞u cho t√¨m ki·∫øm vector similarity v·ªõi h√†ng tri·ªáu vectors.

**Index Type**: `IndexFlatIP` (Inner Product)
- Ph√π h·ª£p v·ªõi **cosine similarity** v√¨ embeddings ƒë√£ ƒë∆∞·ª£c normalize L2
- Khi ||a|| = ||b|| = 1, th√¨: `cosine_similarity(a, b) = dot_product(a, b)`

#### 3.1.2. Quy tr√¨nh

```
1. Pre-compute embeddings (Offline)
   - Encode t·∫•t c·∫£ s·∫£n ph·∫©m ‚Üí embeddings (256D)
   - L∆∞u v√†o FAISS index
   - L∆∞u metadata (product IDs, URLs) v√†o NPZ file

2. Query (Online)
   - Encode query product ‚Üí embedding (256D)
   - Search FAISS index: index.search(query_embedding, k=50)
   - Nh·∫≠n v·ªÅ: similarities array, indices array

3. Map to Products
   - D√πng indices ƒë·ªÉ map v·ªÅ product IDs
   - Fetch product details t·ª´ database
   - Return top-K products
```

#### 3.1.3. C√¥ng th·ª©c

**Inner Product Search**:
```
similarity[i] = dot_product(query_embedding, index_embedding[i])
```

V√¨ embeddings ƒë√£ normalize L2:
```
similarity[i] = cosine_similarity(query_embedding, index_embedding[i])
```

**Range**: [-1, 1] (sau khi normalize, th·ª±c t·∫ø l√† [0, 1] v√¨ embeddings l√† non-negative)

#### 3.1.4. Performance

- **Index size**: ~N vectors (N = s·ªë s·∫£n ph·∫©m)
- **Query time**: O(log N) v·ªõi approximate methods, O(N) v·ªõi IndexFlatIP
- **Memory**: ~256 bytes per vector
- **Latency**: < 10ms cho queries v·ªõi h√†ng ngh√¨n vectors

### 3.2. Cosine Similarity

#### 3.2.1. C√¥ng th·ª©c

**Cosine Similarity**:
```
similarity(A, B) = dot_product(A, B) / (||A|| * ||B||)
```

V√¨ embeddings ƒë√£ normalize L2 (||A|| = ||B|| = 1):
```
similarity(A, B) = dot_product(A, B)
```

#### 3.2.2. √ù nghƒ©a

- **1.0**: Ho√†n to√†n gi·ªëng nhau (c√πng h∆∞·ªõng)
- **0.0**: Tr·ª±c giao (kh√¥ng li√™n quan)
- **-1.0**: Ho√†n to√†n ng∆∞·ª£c nhau (kh√¥ng x·∫£y ra v·ªõi embeddings ƒë√£ normalize)

#### 3.2.3. Implementation

```python
# V√¨ embeddings ƒë√£ normalize L2
similarity = np.dot(embedding_A, embedding_B)
# Range: [0, 1] (th·ª±c t·∫ø v·ªõi fashion embeddings)
```

### 3.3. Aggregation-based Personalized Retrieval

#### 3.3.1. M√¥ t·∫£
Thu·∫≠t to√°n g·ªôp k·∫øt qu·∫£ t·ª´ nhi·ªÅu seed items (s·∫£n ph·∫©m ng∆∞·ªùi d√πng ƒë√£ xem/th√™m gi·ªè/mua) ƒë·ªÉ t·∫°o personalized recommendations.

#### 3.3.2. Quy tr√¨nh

```
INPUT: recent_item_ids = [id1, id2, ..., id10]  (t·ªëi ƒëa 10 items)

1. V·ªõi m·ªói seed_id trong recent_item_ids:
   a. T√¨m top-K similar products (K=50) b·∫±ng FAISS
   b. L·∫•y similarity scores
   c. L∆∞u v√†o embedding_scores[product_id] = max(similarity)

2. AGGREGATION:
   - V·ªõi m·ªói product xu·∫•t hi·ªán trong nhi·ªÅu seed results:
     - L·∫•y MAX similarity score
     - aggregate_scores[product_id] = max(all_similarities)

3. RANKING:
   - S·∫Øp x·∫øp aggregate_scores theo th·ª© t·ª± gi·∫£m d·∫ßn
   - L·∫•y top-N (limit) products

OUTPUT: Danh s√°ch candidates v·ªõi aggregated scores
```

#### 3.3.3. C√¥ng th·ª©c

**MAX Aggregation**:
```
aggregate_score[product_id] = max(similarity(seed_i, product_id)) 
                                for all seed_i in recent_item_ids
```

**V√≠ d·ª•**:
```
Seed 1 ‚Üí [Product A: 0.85, Product B: 0.80]
Seed 2 ‚Üí [Product A: 0.90, Product C: 0.75]
Seed 3 ‚Üí [Product B: 0.88, Product C: 0.70]

Aggregate:
- Product A: max(0.85, 0.90) = 0.90
- Product B: max(0.80, 0.88) = 0.88
- Product C: max(0.75, 0.70) = 0.75

Ranking (top-2):
1. Product A: 0.90
2. Product B: 0.88
```

#### 3.3.4. ∆Øu ƒëi·ªÉm
- ‚úÖ ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu
- ‚úÖ Hi·ªáu qu·∫£ v·ªõi √≠t seed items
- ‚úÖ T·∫≠n d·ª•ng visual similarity

#### 3.3.5. H·∫°n ch·∫ø
- ‚ö†Ô∏è Ch·ªâ d√πng MAX, kh√¥ng weighted average
- ‚ö†Ô∏è Kh√¥ng xem x√©t th·ª© t·ª±/t·∫ßn su·∫•t t∆∞∆°ng t√°c
- ‚ö†Ô∏è Cold start: Ng∆∞·ªùi d√πng m·ªõi ch∆∞a c√≥ seed items

### 3.4. Hybrid Scoring (Retrieval Stage)

#### 3.4.1. M√¥ t·∫£
K·∫øt h·ª£p **embedding similarity**, **popularity**, v√† **user affinity** ƒë·ªÉ t·∫°o hybrid score cho personalized retrieval.

#### 3.4.2. C√¥ng th·ª©c

**Hybrid Score**:
```
hybrid_score = Œ±¬∑normalized_embedding + Œ≤¬∑normalized_popularity + Œ≥¬∑normalized_affinity
```

Trong ƒë√≥:
- `Œ±` (alpha): Weight cho embedding similarity (default: 0.6)
- `Œ≤` (beta): Weight cho popularity (default: 0.3)
- `Œ≥` (gamma): Weight cho user affinity (default: 0.1)
- T·ªïng weights ƒë∆∞·ª£c normalize v·ªÅ 1.0: `Œ± + Œ≤ + Œ≥ = 1.0`

#### 3.4.3. Normalization

**Min-Max Normalization**:
```
normalized_score = (score - min_score) / (max_score - min_score)
```

√Åp d·ª•ng cho t·ª´ng component:
- `normalized_embedding`: Normalize embedding similarity scores v·ªÅ [0, 1]
- `normalized_popularity`: Normalize popularity scores v·ªÅ [0, 1]
- `normalized_affinity`: Normalize user affinity scores v·ªÅ [0, 1]

#### 3.4.4. Data Sources

**1. Embedding Similarity**:
- Source: FAISS search t·ª´ seed items
- Range: [0, 1] (cosine similarity)

**2. Popularity**:
- Source: Events aggregation (`/api/events/aggregates/popularity`)
- Calculation: Weighted sum c·ªßa events (view, add_to_cart, purchase)
- Weights: view=1, add_to_cart=2, purchase=5
- Formula: `popularity = view_count + 2√óadd_to_cart_count + 5√ópurchase_count`

**3. User Affinity**:
- Source: Events aggregation (`/api/events/aggregates/affinity?userId=...`)
- Calculation: Weighted sum c·ªßa user's events v·ªõi t·ª´ng item
- Weights: view=1, add_to_cart=2, purchase=5
- Formula: `affinity[item] = sum(weight √ó event_count) for user's events with item`

#### 3.4.5. Quy tr√¨nh

```
1. Fetch Embedding Similarity
   - V·ªõi m·ªói seed item, t√¨m top-50 similar products (FAISS)
   - Aggregate b·∫±ng MAX: embedding_scores[product_id] = max(similarity)

2. Fetch Popularity Scores
   - Call API: GET /api/events/aggregates/popularity
   - Nh·∫≠n v·ªÅ: {itemId: popularity_score}
   - popularity_scores[item_id] = score

3. Fetch User Affinity (n·∫øu c√≥ userId)
   - Call API: GET /api/events/aggregates/affinity?userId=...
   - Nh·∫≠n v·ªÅ: {itemId: affinity_score}
   - user_affinity_scores[item_id] = score

4. Normalize Scores
   - normalized_embedding = min_max_normalize(embedding_scores)
   - normalized_popularity = min_max_normalize(popularity_scores)
   - normalized_affinity = min_max_normalize(user_affinity_scores)

5. Compute Hybrid Scores
   - V·ªõi m·ªói product_id:
     hybrid_score = Œ±¬∑normalized_embedding + Œ≤¬∑normalized_popularity + Œ≥¬∑normalized_affinity

6. Rank v√† Return
   - S·∫Øp x·∫øp theo hybrid_score gi·∫£m d·∫ßn
   - Return top-N candidates
```

#### 3.4.6. Fallback Strategy

**N·∫øu kh√¥ng c√≥ recent items**:
1. Fallback to popularity-only ranking
2. N·∫øu kh√¥ng c√≥ popularity, fallback to index order

**N·∫øu kh√¥ng c√≥ userId**:
- B·ªè qua user affinity (Œ≥ = 0)
- Ch·ªâ d√πng embedding + popularity

**N·∫øu API calls fail**:
- Graceful degradation: B·ªè qua component failed
- Adjust weights ƒë·ªÉ t·ªïng v·∫´n = 1.0

#### 3.4.7. V√≠ d·ª•

```python
# Input
recent_item_ids = ["product-1", "product-2"]
userId = "user-123"
alpha = 0.6, beta = 0.3, gamma = 0.1

# Scores (before normalization)
embedding_scores = {"product-A": 0.85, "product-B": 0.70}
popularity_scores = {"product-A": 45.5, "product-B": 120.0}
affinity_scores = {"product-A": 12.3, "product-B": 5.0}

# Normalized scores
normalized_embedding = {"product-A": 1.0, "product-B": 0.0}  # min=0.70, max=0.85
normalized_popularity = {"product-A": 0.0, "product-B": 1.0}  # min=45.5, max=120.0
normalized_affinity = {"product-A": 1.0, "product-B": 0.0}    # min=5.0, max=12.3

# Hybrid scores
hybrid_A = 0.6√ó1.0 + 0.3√ó0.0 + 0.1√ó1.0 = 0.7
hybrid_B = 0.6√ó0.0 + 0.3√ó1.0 + 0.1√ó0.0 = 0.3

# Ranking
1. Product A: 0.7
2. Product B: 0.3
```

---

## 4. THU·∫¨T TO√ÅN SCORING & RANKING

### 4.1. Business Rules Engine (Soft Scoring)

#### 4.1.1. M√¥ t·∫£
Thay v√¨ lo·∫°i b·ªè s·∫£n ph·∫©m (hard filter), h·ªá th·ªëng ƒëi·ªÅu ch·ªânh ƒëi·ªÉm similarity d·ª±a tr√™n c√°c quy t·∫Øc kinh doanh (soft scoring).

#### 4.1.2. Hard Filter: Category Matching

**Quy t·∫Øc**: Ch·ªâ gi·ªØ l·∫°i s·∫£n ph·∫©m c√πng category v·ªõi target product.

**Fields ki·ªÉm tra**:
- `articleType`
- `masterCategory`
- `subCategory`

**Logic**: T·∫•t c·∫£ 3 fields ph·∫£i kh·ªõp (n·∫øu c√≥ gi√° tr·ªã).

**V√≠ d·ª•**:
```
Target: {articleType: "T-Shirt", masterCategory: "Apparel", subCategory: "Topwear"}
Candidate: {articleType: "T-Shirt", masterCategory: "Apparel", subCategory: "Topwear"} ‚úÖ PASS
Candidate: {articleType: "Jeans", masterCategory: "Apparel", subCategory: "Bottomwear"} ‚ùå FILTERED OUT
```

#### 4.1.3. Soft Scoring: Price

**Quy t·∫Øc**:
- **Trong kho·∫£ng gi√°** (¬±tolerance): **+10% boost**
- **Ngo√†i kho·∫£ng gi√°**: **-5% penalty**

**C√¥ng th·ª©c**:
```
tolerance = price_tolerance (default: 0.5 = ¬±50%)
min_price = target_price √ó (1 - tolerance)
max_price = target_price √ó (1 + tolerance)

if min_price ‚â§ product_price ‚â§ max_price:
    adjusted_score = min(base_score + 0.10, 1.0)
else:
    adjusted_score = max(base_score - 0.05, 0.0)
```

**V√≠ d·ª•**:
```
target_price = 500,000 VND
tolerance = 0.5
min_price = 250,000 VND
max_price = 750,000 VND

Product A: price = 600,000 VND ‚Üí Trong kho·∫£ng ‚Üí +10% boost
Product B: price = 1,000,000 VND ‚Üí Ngo√†i kho·∫£ng ‚Üí -5% penalty
```

#### 4.1.4. Soft Scoring: Gender

**Quy t·∫Øc**:
- **Exact match** (Male-Male, Female-Female): **+8% boost**
- **Unisex** (m·ªôt trong hai l√† Unisex): **+5% boost**
- **Mismatch** (Male-Female): **-10% penalty**

**C√¥ng th·ª©c**:
```
if product_gender == target_gender:
    adjusted_score = min(base_score + 0.08, 1.0)
elif product_gender == "Unisex" or target_gender == "Unisex":
    adjusted_score = min(base_score + 0.05, 1.0)
else:
    adjusted_score = max(base_score - 0.10, 0.0)
```

#### 4.1.5. Soft Scoring: Usage

**Quy t·∫Øc**:
- **Exact match** (Casual-Casual, Formal-Formal): **+8% boost**
- **Casual fallback** (m·ªôt trong hai l√† Casual): **+3% boost**
- **Mismatch**: **-8% penalty**

**C√¥ng th·ª©c**:
```
if product_usage == target_usage:
    adjusted_score = min(base_score + 0.08, 1.0)
elif product_usage == "Casual" or target_usage == "Casual":
    adjusted_score = min(base_score + 0.03, 1.0)
else:
    adjusted_score = max(base_score - 0.08, 0.0)
```

#### 4.1.6. Soft Scoring: Brand

**Quy t·∫Øc**:
- **C√πng brand**: **+5% boost** (c√≥ th·ªÉ c·∫•u h√¨nh)

**C√¥ng th·ª©c**:
```
brand_boost = options.get('brandBoost', 0.05)

if product_brand.lower() == target_brand.lower():
    adjusted_score = min(base_score + brand_boost, 1.0)
```

#### 4.1.7. T·ªïng h·ª£p Soft Scoring

**Quy tr√¨nh**:
```
1. Base score = similarity_score (t·ª´ FAISS ho·∫∑c hybrid scoring)

2. Apply Price Scoring
   adjusted_score = base_score ¬± price_adjustment

3. Apply Gender Scoring
   adjusted_score = adjusted_score ¬± gender_adjustment

4. Apply Usage Scoring
   adjusted_score = adjusted_score ¬± usage_adjustment

5. Apply Brand Boost
   adjusted_score = adjusted_score + brand_boost (n·∫øu c√πng brand)

6. Cap at [0, 1]
   adjusted_score = min(max(adjusted_score, 0.0), 1.0)

7. Filter by min_similarity threshold
   if adjusted_score < min_similarity:
       remove product
```

**V√≠ d·ª• t√≠nh ƒëi·ªÉm**:
```
Base similarity: 0.80

Price: Trong kho·∫£ng ‚Üí +0.10 ‚Üí 0.90
Gender: Exact match ‚Üí +0.08 ‚Üí 0.98
Usage: Exact match ‚Üí +0.08 ‚Üí 1.0 (cap)
Brand: C√πng brand ‚Üí +0.05 ‚Üí 1.0 (cap)

Final score: 1.0
```

### 4.2. Rank and Limit

#### 4.2.1. M√¥ t·∫£
S·∫Øp x·∫øp s·∫£n ph·∫©m theo adjusted score v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£.

#### 4.2.2. Quy tr√¨nh

```
1. Sort by adjusted_score (descending)
   sorted_products = sort(products, key=lambda p: -p.adjusted_score)

2. Limit to top-N
   top_products = sorted_products[:limit]

3. Return results
```

#### 4.2.3. Diversity Boost (Optional, ch∆∞a tri·ªÉn khai)

**M·ª•c ti√™u**: Tr√°nh hi·ªÉn th·ªã qu√° nhi·ªÅu s·∫£n ph·∫©m gi·ªëng nhau (c√πng brand, c√πng color).

**Thu·∫≠t to√°n** (s·∫Ω tri·ªÉn khai):
- MMR (Maximal Marginal Relevance)
- xQuAD (eXtended Query Aspect Diversification)

---

## 5. BUSINESS RULES & FILTERING

### 5.1. Category Pre-filtering

#### 5.1.1. M√¥ t·∫£
L·ªçc s·∫£n ph·∫©m theo category TR∆Ø·ªöC KHI th·ª±c hi·ªán AI search ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng candidates.

#### 5.1.2. Quy tr√¨nh

```
1. Extract target category
   target_category = {
       articleType: target.articleType,
       masterCategory: target.masterCategory,
       subCategory: target.subCategory
   }

2. Pre-filter candidates
   candidate_pool = all_products.filter(
       product.articleType == target.articleType AND
       product.masterCategory == target.masterCategory AND
       product.subCategory == target.subCategory
   )

3. Apply AI search on filtered pool
   results = FAISS_search(candidate_pool, query_embedding)
```

#### 5.1.3. ∆Øu ƒëi·ªÉm
- ‚úÖ Gi·∫£m s·ªë l∆∞·ª£ng candidates c·∫ßn x·ª≠ l√Ω
- ‚úÖ TƒÉng t·ªëc ƒë·ªô search
- ‚úÖ ƒê·∫£m b·∫£o recommendations c√πng category

#### 5.1.4. H·∫°n ch·∫ø
- ‚ö†Ô∏è C√≥ th·ªÉ b·ªè s√≥t cross-category recommendations
- ‚ö†Ô∏è Kh√¥ng linh ho·∫°t cho users mu·ªën explore

### 5.2. Stock Filtering

#### 5.2.1. M√¥ t·∫£
Ch·ªâ recommend s·∫£n ph·∫©m c√≤n h√†ng (c√≥ √≠t nh·∫•t m·ªôt variant active v·ªõi stock > 0).

#### 5.2.2. Quy tr√¨nh

```
1. Check product variants
   has_stock = any(variant.status == "Active" AND variant.stock > 0 
                   for variant in product.variants)

2. Filter out out-of-stock products
   if not has_stock:
       remove product from recommendations
```

### 5.3. Price Range Filtering (Optional)

#### 5.3.1. M√¥ t·∫£
C√≥ th·ªÉ filter theo kho·∫£ng gi√° (ch∆∞a tri·ªÉn khai nh∆∞ hard filter, ch·ªâ c√≥ soft scoring).

#### 5.3.2. S·∫Ω tri·ªÉn khai
- Hard filter: Lo·∫°i b·ªè s·∫£n ph·∫©m ngo√†i kho·∫£ng gi√°
- User preference: L∆∞u price range preference c·ªßa user

---

## 6. M√î H√åNH V√Ä THU·∫¨T TO√ÅN S·∫º TRI·ªÇN KHAI

### 6.1. Learning-to-Rank (Phase 4)

#### 6.1.1. M√¥ t·∫£
S·ª≠ d·ª•ng machine learning model (XGBoost/LightGBM) ƒë·ªÉ s·∫Øp x·∫øp candidates t·ªëi ∆∞u CTR/ATC/Conversion.

#### 6.1.2. Ki·∫øn tr√∫c

**Model**: XGBoost ho·∫∑c LightGBM
- **Type**: Gradient Boosting Decision Trees
- **Task**: Ranking (pointwise, pairwise, ho·∫∑c listwise)

#### 6.1.3. Features

**1. User Features**:
- `category_affinity`: Affinity v·ªõi category (t·ª´ events)
- `brand_affinity`: Affinity v·ªõi brand (t·ª´ events)
- `price_sensitivity`: Average spend (t·ª´ orders)
- `recency`: Th·ªùi gian k·ªÉ t·ª´ l·∫ßn t∆∞∆°ng t√°c cu·ªëi
- `frequency`: T·∫ßn su·∫•t t∆∞∆°ng t√°c

**2. Item Features**:
- `price`: Gi√° s·∫£n ph·∫©m
- `category`: Category information (one-hot encoded)
- `brand`: Brand (one-hot encoded)
- `popularity`: Popularity score (t·ª´ events)
- `stock`: S·ªë l∆∞·ª£ng t·ªìn kho
- `age`: Th·ªùi gian k·ªÉ t·ª´ khi t·∫°o s·∫£n ph·∫©m

**3. Context Features**:
- `time_of_day`: Gi·ªù trong ng√†y (0-23)
- `day_of_week`: Ng√†y trong tu·∫ßn (0-6)
- `device`: Thi·∫øt b·ªã (web, mobile)
- `geo`: ƒê·ªãa l√Ω (VN, US, ...)

**4. Cross Features**:
- `similarity_score`: Embedding similarity (t·ª´ retrieval)
- `same_category`: Boolean (c√πng category v·ªõi seed item)
- `same_brand`: Boolean (c√πng brand v·ªõi seed item)
- `price_diff`: Ch√™nh l·ªách gi√° v·ªõi seed item

#### 6.1.4. Training Pipeline

**1. Labeled Dataset**:
- **Positive labels**: Clicks, add_to_cart, purchase
- **Negative labels**: Impressions without interactions
- **Source**: Events data (`events` collection)

**2. Feature Engineering**:
- Aggregate events ƒë·ªÉ t√≠nh user/item features
- Join v·ªõi product data ƒë·ªÉ l·∫•y item features
- Extract context t·ª´ event context field

**3. Training**:
- Split: Train (80%), Validation (10%), Test (10%)
- Metrics: AUC, NDCG@K, MRR
- Hyperparameter tuning: Grid search ho·∫∑c Bayesian optimization

**4. Serving**:
- Load model via ONNX/TorchScript
- Latency target: < 50ms per request
- Batch prediction cho multiple candidates

#### 6.1.5. C√¥ng th·ª©c

**Pointwise Ranking**:
```
score = ML_Model(user_features, item_features, context_features, cross_features)
```

**Pairwise Ranking** (n·∫øu d√πng):
```
P(item_A > item_B) = sigmoid(ML_Model(features_A) - ML_Model(features_B))
```

**Listwise Ranking** (n·∫øu d√πng):
```
scores = ML_Model(user_features, [item_features_1, item_features_2, ...])
```

#### 6.1.6. API Endpoint

```
POST /api/recommendations/rank
Body: {
  "userId": "user-123",
  "candidates": [
    {
      "productId": "product-1",
      "features": {...}
    },
    ...
  ],
  "context": {
    "device": "web",
    "timeOfDay": 14,
    "geo": "VN"
  }
}

Response: {
  "ranked": [
    {
      "productId": "product-1",
      "score": 0.85,
      "rank": 1
    },
    ...
  ]
}
```

### 6.2. Diversification (Phase 5)

#### 6.2.1. MMR (Maximal Marginal Relevance)

**M·ª•c ti√™u**: Ch·ªçn items v·ª´a relevant v·ª´a diverse.

**C√¥ng th·ª©c**:
```
MMR(item) = Œª¬∑relevance(item) - (1-Œª)¬∑max_similarity(item, selected_items)
```

Trong ƒë√≥:
- `Œª`: Balance parameter (0 = ch·ªâ diversity, 1 = ch·ªâ relevance)
- `relevance(item)`: Relevance score (t·ª´ ranking model)
- `max_similarity(item, selected_items)`: ƒê·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t v·ªõi items ƒë√£ ch·ªçn

**Quy tr√¨nh**:
```
1. Initialize: selected = [], candidates = [all_items]

2. Select first item:
   selected.append(candidates.pop(max_relevance_item))

3. For remaining slots:
   For each candidate in candidates:
       mmr_score = Œª¬∑relevance(candidate) - (1-Œª)¬∑max_similarity(candidate, selected)
   selected.append(candidates.pop(max_mmr_item))

4. Return selected
```

#### 6.2.2. xQuAD (eXtended Query Aspect Diversification)

**M·ª•c ti√™u**: ƒêa d·∫°ng h√≥a theo c√°c aspects c·ªßa query (category, brand, price range).

**C√¥ng th·ª©c**:
```
xQuAD(item) = (1-Œª)¬∑relevance(item) + Œª¬∑coverage(item, selected)
```

Trong ƒë√≥:
- `coverage(item, selected)`: ƒê·ªô bao ph·ªß aspects ch∆∞a ƒë∆∞·ª£c cover b·ªüi selected items
- `Œª`: Balance parameter

**Aspects**:
- Category (articleType, masterCategory, subCategory)
- Brand
- Price range (low, medium, high)
- Color
- Style

### 6.3. Collaborative Filtering (Future)

#### 6.3.1. Matrix Factorization

**M·ª•c ti√™u**: H·ªçc latent factors t·ª´ user-item interaction matrix.

**C√¥ng th·ª©c**:
```
R_ui ‚âà P_u ¬∑ Q_i^T
```

Trong ƒë√≥:
- `R_ui`: User-item interaction matrix
- `P_u`: User latent factors (K dimensions)
- `Q_i`: Item latent factors (K dimensions)

**Training**:
- Objective: Minimize reconstruction error
- Regularization: L2 regularization on factors
- Optimization: SGD ho·∫∑c ALS

#### 6.3.2. Neural Collaborative Filtering

**M·ª•c ti√™u**: S·ª≠ d·ª•ng neural networks ƒë·ªÉ h·ªçc user-item interactions.

**Ki·∫øn tr√∫c**:
```
User Embedding (K dim) + Item Embedding (K dim)
  ‚Üì
Concatenate
  ‚Üì
MLP (Multi-Layer Perceptron)
  ‚Üì
Output: Interaction Score
```

### 6.4. A/B Testing Framework (Phase 6)

#### 6.4.1. M√¥ t·∫£
So s√°nh hi·ªáu qu·∫£ c·ªßa c√°c m√¥ h√¨nh/thu·∫≠t to√°n recommendation.

#### 6.4.2. Metrics

**Online Metrics**:
- **CTR** (Click-Through Rate): `clicks / impressions`
- **ATC** (Add-to-Cart Rate): `add_to_cart / impressions`
- **Conversion Rate**: `purchases / impressions`
- **Revenue**: Total revenue from recommendations

**Offline Metrics**:
- **AUC**: Area Under ROC Curve
- **NDCG@K**: Normalized Discounted Cumulative Gain at K
- **MRR**: Mean Reciprocal Rank

#### 6.4.3. Framework

**Feature Flags**:
- S·ª≠ d·ª•ng feature flag service (Unleash, LaunchDarkly)
- Route % traffic gi·ªØa model A v√† model B
- Example: 50% traffic ‚Üí Model A, 50% ‚Üí Model B

**Tracking**:
- Log model version cho m·ªói recommendation request
- Track metrics per model version
- Statistical significance testing (t-test, chi-square test)

### 6.5. Cold Start Solutions

#### 6.5.1. New User Cold Start

**Strategies**:
1. **Popular Items**: Recommend top popular items
2. **Demographic-based**: Recommend based on user demographics (age, gender, location)
3. **Content-based**: Recommend based on user's initial preferences (n·∫øu c√≥)
4. **Hybrid**: K·∫øt h·ª£p multiple strategies

#### 6.5.2. New Item Cold Start

**Strategies**:
1. **Content-based**: D√πng visual similarity (FashionCLIP)
2. **Category-based**: Recommend trong c√πng category
3. **Popular boost**: TƒÉng popularity score cho new items
4. **Exploration**: ∆Øu ti√™n new items trong recommendations

---

## 7. SO S√ÅNH V√Ä ƒê√ÅNH GI√Å

### 7.1. So s√°nh c√°c thu·∫≠t to√°n Retrieval

| Thu·∫≠t to√°n | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Tr·∫°ng th√°i |
|------------|---------|------------|------------|
| **FAISS Vector Search** | ‚úÖ Nhanh, ch√≠nh x√°c, scalable | ‚ö†Ô∏è C·∫ßn pre-compute embeddings | ‚úÖ ƒê√£ tri·ªÉn khai |
| **Hybrid Scoring** | ‚úÖ K·∫øt h·ª£p multiple signals | ‚ö†Ô∏è Ph·ª©c t·∫°p, c·∫ßn tune weights | ‚úÖ ƒê√£ tri·ªÉn khai |
| **Collaborative Filtering** | ‚úÖ T·∫≠n d·ª•ng user behavior | ‚ö†Ô∏è Cold start, sparse data | ‚ùå Ch∆∞a tri·ªÉn khai |

### 7.2. So s√°nh c√°c thu·∫≠t to√°n Ranking

| Thu·∫≠t to√°n | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Tr·∫°ng th√°i |
|------------|---------|------------|------------|
| **Business Rules** | ‚úÖ ƒê∆°n gi·∫£n, interpretable | ‚ö†Ô∏è Rule-based, kh√¥ng h·ªçc t·ª´ data | ‚úÖ ƒê√£ tri·ªÉn khai |
| **Learning-to-Rank** | ‚úÖ H·ªçc t·ª´ data, t·ªëi ∆∞u metrics | ‚ö†Ô∏è C·∫ßn labeled data, ph·ª©c t·∫°p | ‚ùå Ch∆∞a tri·ªÉn khai |
| **Neural Ranking** | ‚úÖ Non-linear patterns | ‚ö†Ô∏è Black box, c·∫ßn nhi·ªÅu data | ‚ùå Ch∆∞a tri·ªÉn khai |

### 7.3. ƒê√°nh gi√° hi·ªáu qu·∫£

#### 7.3.1. Metrics hi·ªán t·∫°i

**Offline Metrics** (c√≥ th·ªÉ t√≠nh t·ª´ events):
- **Coverage**: % s·∫£n ph·∫©m ƒë∆∞·ª£c recommend
- **Diversity**: ƒê·ªô ƒëa d·∫°ng c·ªßa recommendations (brand, category)
- **Popularity Bias**: ƒê·ªô l·ªách v·ªÅ popular items

**Online Metrics** (c·∫ßn tracking):
- **CTR**: Click-through rate
- **ATC**: Add-to-cart rate
- **Conversion**: Purchase rate
- **Revenue**: Revenue t·ª´ recommendations

#### 7.3.2. A/B Testing Plan

**Experiment 1**: Business Rules vs Learning-to-Rank
- Hypothesis: Learning-to-Rank s·∫Ω c·∫£i thi·ªán CTR v√† Conversion
- Metrics: CTR, ATC, Conversion Rate
- Duration: 2 weeks
- Traffic split: 50/50

**Experiment 2**: Hybrid Scoring weights
- Hypothesis: T·ªëi ∆∞u weights (Œ±, Œ≤, Œ≥) s·∫Ω c·∫£i thi·ªán relevance
- Metrics: CTR, User engagement
- Duration: 1 week
- Variations: (0.7, 0.2, 0.1), (0.6, 0.3, 0.1), (0.5, 0.4, 0.1)

### 7.4. Roadmap

#### Phase 3 (Retrieval) - ‚úÖ HO√ÄN TH√ÄNH
- [x] FAISS vector search
- [x] Hybrid scoring (embedding + popularity + affinity)
- [x] Events integration
- [ ] Redis caching (optional optimization)

#### Phase 4 (Ranking) - ‚ùå CH∆ØA B·∫ÆT ƒê·∫¶U
- [ ] Feature engineering pipeline
- [ ] Learning-to-Rank model (XGBoost/LightGBM)
- [ ] Training pipeline t·ª´ events data
- [ ] Model serving (ONNX/TorchScript)
- [ ] A/B testing framework

#### Phase 5 (Orchestration) - ‚ùå CH∆ØA B·∫ÆT ƒê·∫¶U
- [ ] Diversification (MMR/xQuAD)
- [ ] Business rules integration
- [ ] Caching strategy (Redis)
- [ ] Orchestrated endpoint (`/api/recommendations`)

#### Phase 6-7 (A/B Testing & Privacy) - ‚ùå CH∆ØA B·∫ÆT ƒê·∫¶U
- [ ] Feature flags integration
- [ ] Metrics dashboard
- [ ] Privacy compliance (GDPR, opt-out)
- [ ] Data retention policies

---

## 8. T√ÄI LI·ªÜU THAM KH·∫¢O

### 8.1. Papers

1. **CLIP**: "Learning Transferable Visual Models From Natural Language Supervision" (Radford et al., 2021)
2. **FAISS**: "Billion-scale similarity search with GPUs" (Johnson et al., 2019)
3. **Learning-to-Rank**: "From RankNet to LambdaRank to LambdaMART: An Overview" (Burges, 2010)
4. **MMR**: "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries" (Carbonell & Goldstein, 1998)
5. **xQuAD**: "Diversifying Search Results" (Santos et al., 2010)

### 8.2. Documentation

- **FashionCLIP**: `backend/fashion-service/models/FashionCLIP.py`
- **Recommendation Service**: `backend/fashion-service/services/recommendation_service.py`
- **Business Rules**: `backend/fashion-service/utils/filters.py`
- **Events API Client**: `backend/fashion-service/utils/events_api_client.py`

### 8.3. External Resources

- **FAISS**: https://github.com/facebookresearch/faiss
- **CLIP**: https://github.com/openai/CLIP
- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/

---

**T√°c gi·∫£**: AI Assistant  
**Ng√†y t·∫°o**: 2025-01-XX  
**Phi√™n b·∫£n**: 1.0  
**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi**: 2025-01-XX

